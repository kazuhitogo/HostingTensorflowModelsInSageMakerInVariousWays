{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U jedi==0.17.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, os, tarfile, json, numpy as np, base64, sagemaker, boto3\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "from io import BytesIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from time import sleep\n",
    "sm_client = boto3.client('sagemaker')\n",
    "smr_client = boto3.client('sagemaker-runtime')\n",
    "sm_role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用するモデルのロードと動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.applications.mobilenet.MobileNet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = f'{os.getcwd()}/work/'\n",
    "\n",
    "os.makedirs(work_dir, exist_ok=True)\n",
    "# サンプル画像をダウンロード\n",
    "file = tf.keras.utils.get_file(\n",
    "    f'{work_dir}cat.jpg',\n",
    "    'https://gahag.net/img/201608/11s/gahag-0115329292-1.jpg')\n",
    "\n",
    "# 分類クラスをダウンロード\n",
    "labels_path = tf.keras.utils.get_file(\n",
    "    f'{work_dir}/ImageNetLabels.txt',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\n",
    "labels = list(np.array(open(labels_path).read().splitlines())[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./code/labels.txt','wt') as f:\n",
    "    for txt in labels:\n",
    "        f.write(txt+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(file).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像のresizeと前処理結果の確認\n",
    "x,y = 900-537,0\n",
    "img = Image.open(file).crop((x,y,900,537)).resize((model.input_shape[1],model.input_shape[2]))\n",
    "img_arr = ((np.array(img)-127.5)/127.5).astype(np.float32).reshape(-1,model.input_shape[1],model.input_shape[2],3)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 画像のresizeと前処理結果の確認\n",
    "# img = Image.open(file).resize((model.input_shape[1],model.input_shape[2]))\n",
    "# img_arr = ((np.array(img)-127.5)/127.5).astype(np.float32).reshape(-1,model.input_shape[1],model.input_shape[2],3)\n",
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの動作確認\n",
    "print(labels[np.argmax(model.predict(img_arr))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 保存ディレクトリを指定\n",
    "model_dir = './mobilenet/0001'\n",
    "\n",
    "# tar.gz の出力先を指定\n",
    "tar_dir = 'MyModel'\n",
    "os.makedirs(tar_dir, exist_ok=True)\n",
    "tar_name = os.path.join(tar_dir, 'model.tar.gz')\n",
    "\n",
    "# モデルを SavedModel 形式で保存\n",
    "model.save(model_dir)\n",
    "\n",
    "# tar.gz ファイルを出力\n",
    "with tarfile.open(tar_name, mode='w:gz') as tar:\n",
    "    tar.add(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 にアップロードして、返り値としてS3のURIを受け取る\n",
    "model_s3_path = f's3://{bucket}/{tar_dir}'\n",
    "\n",
    "model_s3_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path = tar_name,\n",
    "    desired_s3_uri = model_s3_path\n",
    ")\n",
    "\n",
    "print(model_s3_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sagemaker SDK でマネージドコンテナの URI を取得\n",
    "container_image_tf24_uri = sagemaker.image_uris.retrieve(\n",
    "    \"tensorflow\",  # TensorFlow のマネージドコンテナを利用\n",
    "    sagemaker.session.Session().boto_region_name, # ECR のリージョンを指定\n",
    "    version='2.4', # TensorFlow のバージョンを指定\n",
    "    instance_type = 'ml.m5.large', # インスタンスタイプを指定\n",
    "    image_scope = 'inference' # 推論コンテナを指定\n",
    ")\n",
    "\n",
    "print(container_image_tf24_uri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Python SDK で Hosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'MyTFModelFromSMSDK'\n",
    "endpoint_config_name = model_name + 'Endpoint'\n",
    "endpoint_name = endpoint_config_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# モデルとコンテナの指定\n",
    "tf_model = TensorFlowModel(\n",
    "    name = model_name,\n",
    "    model_data=model_s3_uri, # モデルの S3 URI\n",
    "    role= sm_role, # 割り当てるロール\n",
    "    image_uri = container_image_tf24_uri, # コンテナイメージの S3 URI\n",
    ")\n",
    "# デプロイ(endpoint 生成)\n",
    "predictor = tf_model.deploy(\n",
    "    endpoint_name=endpoint_name, # エンドポイントの名前\n",
    "    initial_instance_count=1, # インスタンス数\n",
    "    instance_type='ml.m5.large', # インスタンスタイプ\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(file).resize((model.input_shape[1],model.input_shape[2]))\n",
    "img_arr = ((np.array(img)-127.5)/127.5).astype(np.float32).reshape(-1,model.input_shape[1],model.input_shape[2],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = np.argmax(predictor.predict(img_arr)['predictions'][0])\n",
    "print(labels[result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "r = sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "r = sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boto3 で Hosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'MyTFModelAddProcessFromBoto3'\n",
    "endpoint_config_name = model_name + 'EndpointConfig'\n",
    "endpoint_name = model_name + 'Endpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        # SageMaker SDK の時と同じ URI を指定\n",
    "        'Image': container_image_tf24_uri,\n",
    "        # SageMaker SDK の時と同じ URI を指定\n",
    "        'ModelDataUrl': model_s3_uri,\n",
    "    },\n",
    "    # SageMaker SDK の時と同じ role を指定\n",
    "    ExecutionRoleArn=sm_role,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'AllTrafic',\n",
    "            'ModelName': model_name,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': 'ml.m5.xlarge',\n",
    "        },\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")\n",
    "while True:\n",
    "    status = sm_client.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n",
    "    if status in ['InService','RollingBack','SystemUpdating','OutOfService']:\n",
    "        print('!')\n",
    "        print(status)\n",
    "        break\n",
    "    else:\n",
    "        print('.',end='')\n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リストを文字列にして渡すパターン\n",
    "request_args = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'ContentType' : 'application/json',\n",
    "    'Accept' : 'application/json',\n",
    "    'Body' : str(img_arr.tolist())\n",
    "}\n",
    "response = smr_client.invoke_endpoint(**request_args)\n",
    "predictions = json.loads(response['Body'].read().decode('utf-8'))['predictions'][0]\n",
    "print(labels[np.argmax(predictions)],predictions[np.argmax(predictions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonにして渡すパターン\n",
    "request_args = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'ContentType' : 'application/json',\n",
    "    'Accept' : 'application/json',\n",
    "    'Body' : json.dumps({\"instances\": img_arr.tolist()})\n",
    "}\n",
    "response = smr_client.invoke_endpoint(**request_args)\n",
    "predictions = json.loads(response['Body'].read().decode('utf-8'))['predictions'][0]\n",
    "print(labels[np.argmax(predictions)],predictions[np.argmax(predictions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "r = sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "r = sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理/後処理追加\n",
    "* リスト形式でデータを作成し(た後で json形式に変換し)て predict を行うが、 inference.py を使うことで前処理/後処理を endpoint 側うことも可能。\n",
    "  * 重い画像の前処理を潤沢なエンドポイントのコンピューティングリソースで実行することで、呼び出し側　(Lambda など)の頻繁かつ長期的に処理するコンピューティングリソースのスペックを低減できる\n",
    "  * 呼び出し側が前処理を意識せずに実装できるようになる(呼び出し側はデータサイエンティストの領域に入らずに済み、エンドポイントで実行する前処理までをDSの領域にできる）\n",
    "* 以下を例に実装する。  \n",
    "    * 前処理の例）画像分類であれば、画像のバイナリデータを base64 エンコーディングしたものを直接送りつけて、 endpoint 側でリストに変換可能\n",
    "    * 後処理の例）softmax の結果から一番可能性の高い値を取得し、そのインデックスからラベルに変換する\n",
    "\n",
    "### SageMaker Python SDK でホスティング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./code/inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'MyTFModelAddProcessFromSMSDK'\n",
    "endpoint_config_name = model_name + 'Endpoint'\n",
    "endpoint_name = endpoint_config_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tar.gz の出力先を指定\n",
    "tar_dir = 'MyModelAddProcess'\n",
    "code_dir = './code'\n",
    "os.makedirs(tar_dir, exist_ok=True)\n",
    "tar_name = os.path.join(tar_dir, 'model.tar.gz')\n",
    "with tarfile.open(tar_name, mode='w:gz') as tar:\n",
    "    tar.add(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_add_process_s3_path = f's3://{bucket}/{tar_dir}'\n",
    "\n",
    "model_add_process_s3_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path = tar_name,\n",
    "    desired_s3_uri = model_add_process_s3_path\n",
    ")\n",
    "print(model_add_process_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inference.py と必要なファイルの設定\n",
    "* entry_point 引数で `inference.py` (名前固定)を指定すると `input_handler` と `output_handler` を推論前後に実行してくれる\n",
    "* 必要なモジュール等がある場合は `source_dir` 引数に格納してあるディレクトリを指定すると一緒に読み込むが、 inference.py が `source_dir` のルートに存在する必要がある\n",
    "* ホスティング先の展開ディレクトリは `/opt/ml/model/code` になるので、テキストファイルを読み込む時は絶対パスで指定するとよい（カレントディレクトリは `/sagemaker` で実行される）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# モデルとコンテナの指定\n",
    "tf_model = TensorFlowModel(\n",
    "    name = model_name,\n",
    "    model_data=model_add_process_s3_uri, # モデルの S3 URI\n",
    "    role= sm_role, # 割り当てるロール\n",
    "    image_uri = container_image_tf24_uri, # コンテナイメージの S3 URI\n",
    "    entry_point = './code/inference.py',\n",
    "    source_dir = './code/'\n",
    ")\n",
    "# デプロイ(endpoint 生成)\n",
    "predictor = tf_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1, # インスタンス数\n",
    "    instance_type='ml.m5.xlarge', # インスタンスタイプ\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 推論\n",
    "with open('./work/mountains.jpg', 'rb') as img:\n",
    "    data = img.read()\n",
    "bio = BytesIO()\n",
    "bio.write(data)\n",
    "b64_data = base64.b64encode(bio.getvalue()).decode('utf-8')\n",
    "json_b64 = json.dumps({'b64_image':b64_data})\n",
    "request_args = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'ContentType' : 'application/json',\n",
    "    'Accept' : 'application/json',\n",
    "    'Body' : json_b64\n",
    "}\n",
    "response = smr_client.invoke_endpoint(**request_args)\n",
    "print(response['Body'].read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# すでにあった場合の削除\n",
    "r = sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "r = sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "r = sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boto3 でホスティングと推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'MyTFModelAddProcessFromBoto3'\n",
    "endpoint_config_name = model_name + 'EndpointConfig'\n",
    "endpoint_name = model_name + 'Endpoint'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inference.py 他をmodel.tar.gz に同包\n",
    "boto3 から endpoint を作成する場合は、SageMaker SDK のように `entry_point` や `source_dir` の設定ができないため、 必要なファイルは予め `model.tar.gz` に一緒に入れる必要がある  \n",
    "(SageMaker SDK の場合は裏側で自動で `inference.py` などを model.tar.gz に再度固めて s3 にアップロードしてくれている)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.tar.gz にモデルなどを固める\n",
    "tar_dir = 'MyModelAddProcess'\n",
    "code_dir = './code'\n",
    "os.makedirs(tar_dir, exist_ok=True)\n",
    "tar_name = os.path.join(tar_dir, 'model.tar.gz')\n",
    "with tarfile.open(tar_name, mode='w:gz') as tar:\n",
    "    tar.add(model_dir)\n",
    "    tar.add(code_dir) # inference.py などを同包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_add_process_s3_path = f's3://{bucket}/{tar_dir}'\n",
    "\n",
    "model_add_process_s3_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path = tar_name,\n",
    "    desired_s3_uri = model_add_process_s3_path\n",
    ")\n",
    "print(model_add_process_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        # SageMaker SDK の時と同じ URI を指定\n",
    "        'Image': container_image_tf24_uri,\n",
    "        # SageMaker SDK の時と同じ URI を指定\n",
    "        'ModelDataUrl': model_add_process_s3_uri,\n",
    "    },\n",
    "    # SageMaker SDK の時と同じ role を指定\n",
    "    ExecutionRoleArn=sm_role,\n",
    ")\n",
    "response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'AllTrafic',\n",
    "            'ModelName': model_name,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': 'ml.m5.xlarge',\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")\n",
    "\n",
    "while True:\n",
    "    status = sm_client.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n",
    "    if status in ['InService','RollingBack','SystemUpdating','OutOfService']:\n",
    "        print('!')\n",
    "        print(status)\n",
    "        break\n",
    "    else:\n",
    "        print('.',end='')\n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論\n",
    "request_args = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'ContentType' : 'application/json',\n",
    "    'Accept' : 'application/json',\n",
    "    'Body' : json_b64\n",
    "}\n",
    "response = smr_client.invoke_endpoint(**request_args)\n",
    "print(response['Body'].read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 削除\n",
    "r = sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "r = sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "r = sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## マルチモデルエンドポイント\n",
    "* １つの推論インスタンスに複数のモデルをデプロイすることが可能\n",
    "* モデルごとにtar.gzにかためて、S3 の指定プレフィックス直下に配置する\n",
    "* 呼び出す(invoke_endpoint)する際にモデルのファイル名を指定する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル準備と動作確認( mobilenetv2 )\n",
    "* 新しくmobilenetv2を追加し、mobilenetとmobilenetv2の2モデルを１つのエンドポイントでホスティングする準備\n",
    "* それぞれのモデルを {モデル名}.tar.gz に固める\n",
    "* 同じキープレフィックスにアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2 = tf.keras.applications.mobilenet_v2.MobileNetV2()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの動作確認(v1との比較)\n",
    "# mobilenet\n",
    "prediction = model.predict(img_arr)[0]\n",
    "print(prediction[np.argmax(prediction)],labels[np.argmax(prediction)])\n",
    "# mobilenetV2\n",
    "prediction = model2.predict(img_arr)[0]\n",
    "print(prediction[np.argmax(prediction)],labels[np.argmax(prediction)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model3.predict(img_arr)[0]\n",
    "print(prediction[np.argmax(prediction)],labels[np.argmax(prediction)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存ディレクトリを指定\n",
    "model2_dir = './mobilenetv2/0001'\n",
    "\n",
    "# mobilenetv2.tar.gz の出力先を指定\n",
    "tar_dir = 'MyMultiModel'\n",
    "os.makedirs(tar_dir, exist_ok=True)\n",
    "tar_name = os.path.join(tar_dir, 'mobilenetv2.tar.gz')\n",
    "\n",
    "# モデルを SavedModel 形式で保存\n",
    "model2.save(model2_dir)\n",
    "\n",
    "# tar.gz ファイルを出力\n",
    "with tarfile.open(tar_name, mode='w:gz') as tar:\n",
    "    tar.add(model2_dir, arcname=\"0001\")\n",
    "\n",
    "# mobilenet.tar.gz の出力先を指定\n",
    "tar_name = os.path.join(tar_dir, 'mobilenet.tar.gz')\n",
    "# tar.gz ファイルを出力\n",
    "with tarfile.open(tar_name, mode='w:gz') as tar:\n",
    "    tar.add(model_dir, arcname=\"0001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model_s3_path = f's3://{bucket}/{tar_dir}/'\n",
    "\n",
    "!aws s3 cp ./MyMultiModel/ {multi_model_s3_path} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'MyMultiModel'\n",
    "endpoint_config_name = model_name + 'EndpointConfig'\n",
    "endpoint_name = model_name + 'Endpoint'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの作成～エンドポイント作成\n",
    "* シングルモデルのときはtar.gzのパスを指定していたが、マルチモデルのときはモデルを保存しているプレフィックスを指定する\n",
    "* 他はシングルモデルと同じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "        PrimaryContainer={\n",
    "        'Image': container_image_tf24_uri,\n",
    "        'Mode':'MultiModel',\n",
    "        'ModelDataUrl': multi_model_s3_path, # tar.gz を配置している S3 パスを指定\n",
    "    },\n",
    "    ExecutionRoleArn=sm_role,\n",
    ")\n",
    "response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'AllTrafic',\n",
    "            'ModelName': model_name,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': 'ml.m5.xlarge',\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")\n",
    "\n",
    "while True:\n",
    "    status = sm_client.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n",
    "    if status in ['InService','RollingBack','SystemUpdating','OutOfService']:\n",
    "        print('!')\n",
    "        print(status)\n",
    "        break\n",
    "    else:\n",
    "        print('.',end='')\n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### マルチモデルエンドポイントでの推論\n",
    "* `TargetModel` 引数にtar.gzに固めたモデルのファイル名を入れればそのモデルが使用される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mobilenet推論\n",
    "request_args = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'ContentType' : 'application/json',\n",
    "    'Accept' : 'application/json',\n",
    "    'TargetModel' : 'mobilenet.tar.gz',\n",
    "    'Body' : json.dumps({\"instances\": img_arr.tolist()})\n",
    "}\n",
    "response = smr_client.invoke_endpoint(**request_args)\n",
    "predictions = json.loads(response['Body'].read().decode('utf-8'))['predictions'][0]\n",
    "print(labels[np.argmax(predictions)],predictions[np.argmax(predictions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mobilenetv2推論\n",
    "request_args = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'ContentType' : 'application/json',\n",
    "    'Accept' : 'application/json',\n",
    "    'TargetModel' : 'mobilenetv2.tar.gz',\n",
    "    'Body' : json.dumps({\"instances\": img_arr.tolist()})\n",
    "}\n",
    "response = smr_client.invoke_endpoint(**request_args)\n",
    "predictions = json.loads(response['Body'].read().decode('utf-8'))['predictions'][0]\n",
    "print(labels[np.argmax(predictions)],predictions[np.argmax(predictions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの追加\n",
    "* 同じプレフィックス下に新しくモデルを追加すれば追加したモデルで推論可能\n",
    "* ここでは mobilenetv2 を別名に差し替えて(mobilenetv2**_2**)、追加でアップロードしてそちらも機能することを確認する\n",
    "\n",
    "注１）モデルの削除は S3 から削除すればできるが、タイムラグがかなりあるので注意。モデルをホスティングをしているインスタンスからモデルが削除されない限り（コントロールできない領域で、ホスティングしているインスタンスのメモリ/ストレージが不足したときのみ自動で読み込んでいるモデルが削除される）S3から削除したモデルで推論できる。  \n",
    "注２）同様にモデルの更新についても、S3に配置したモデルを上書き保存しても古いモデルがうごき続けてしまう可能性がある。[公式のメッセージ](https://docs.aws.amazon.com/sagemaker/latest/dg/add-models-to-endpoint.html)としては「上書き保存はするな」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ./MyMultiModel/mobilenetv2.tar.gz {multi_model_s3_path}mobilenetv2_2.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mobilenet推論\n",
    "request_args = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'ContentType' : 'application/json',\n",
    "    'Accept' : 'application/json',\n",
    "    'TargetModel' : 'mobilenetv2_2.tar.gz', # 後から追加したモデル\n",
    "    'Body' : json.dumps({\"instances\": img_arr.tolist()})\n",
    "}\n",
    "response = smr_client.invoke_endpoint(**request_args)\n",
    "predictions = json.loads(response['Body'].read().decode('utf-8'))['predictions'][0]\n",
    "print(labels[np.argmax(predictions)],predictions[np.argmax(predictions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "r = sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "r = sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## マルチコンテナエンドポイント\n",
    "* マルチモデルでは同じコンテナで複数のモデルをホスティングしたが、複数のコンテナでそれぞれのモデルをホストすることもできる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sagemaker SDK でマネージドコンテナの URI を取得\n",
    "container_image_tf23_uri = sagemaker.image_uris.retrieve(\n",
    "    \"tensorflow\",  # TensorFlow のマネージドコンテナを利用\n",
    "    sagemaker.session.Session().boto_region_name, # ECR のリージョンを指定\n",
    "    version='2.3', # TensorFlow のバージョンを指定\n",
    "    instance_type = 'ml.m5.large', # インスタンスタイプを指定\n",
    "    image_scope = 'inference' # 推論コンテナを指定\n",
    ")\n",
    "\n",
    "print(container_image_tf23_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf23Container = { 'Image':'container_image_tf23_uri','ContainerHostname': 'tf23Container'}\n",
    "tf24Container = { 'Image':'container_image_tf24_uri','ContainerHostname': 'tf24Container'}\n",
    "inferenceExecutionConfig = {'Mode': 'Direct'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-1:102112518831:image/tensorflow-2.3-cpu-py37-ubuntu18.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
